{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This idea of this notebook is to create a first (simple) DL model using all the same features used in the multinomial logit model.\n",
    "\n",
    "Much inspiration was derived from https://towardsdatascience.com/use-machine-learning-to-predict-horse-racing-4f1111fb6ced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from importlib import reload\n",
    "import deeplearninglib\n",
    "reload(deeplearninglib)\n",
    "from deeplearninglib import *\n",
    "\n",
    "import wandb\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select model to train\n",
    "\n",
    "model_inventory = {'mktprob': {'X_columns': [\"mkt_prob\"],\n",
    "                               'learning_rate': 10e-1,\n",
    "                               'epochs': 50,\n",
    "                               'vacant_stall_indicator': False,\n",
    "                               'bias': True,\n",
    "                               'model_architecture': LinSig},\n",
    "                   'mktprob_soft': {'X_columns': [\"mkt_prob\"],\n",
    "                               'learning_rate': 10e-1,\n",
    "                               'epochs': 50,\n",
    "                               'vacant_stall_indicator': False,\n",
    "                               'bias': True,\n",
    "                               'model_architecture': LinSoft},\n",
    "                    'mktprob_MLR': {'X_columns': [\"mkt_prob\"],\n",
    "                               'learning_rate': 10e-1,\n",
    "                               'epochs': 50,\n",
    "                               'vacant_stall_indicator': False,\n",
    "                               'bias': True,\n",
    "                               'model_architecture': MLR},\n",
    "                   'AlunOwen_v0': {'X_columns': [\"age\", \"sire_sr\", \"dam_sr\", \"trainer_sr\", \"daysLTO\", \"position1_1\", \"position1_2\", \"position1_3\", \"position1_4\", \"position2_1\", \"position2_2\", \"position2_3\", \"position2_4\", \"position3_1\", \"position3_2\", \"position3_3\", \"position3_4\", \"entire\", \"gelding\", \"blinkers\", \"visor\", \"cheekpieces\", \"tonguetie\"],\n",
    "                                   'learning_rate': 10e-3,\n",
    "                                   'epochs': 100,\n",
    "                                   'vacant_stall_indicator': False,\n",
    "                                   'bias': True,\n",
    "                                   'model_architecture': LinSig},\n",
    "                   'AlunOwen_v1': {'X_columns': [\"age\", \"trainer_sr\", \"daysLTO\", \"position1_1\", \"position1_2\", \"position1_3\", \"position1_4\", \"position2_1\", \"position2_2\", \"position2_3\", \"position2_4\", \"position3_1\", \"position3_2\", \"position3_3\", \"position3_4\", \"entire\", \"gelding\", \"blinkers\", \"visor\", \"cheekpieces\", \"tonguetie\"],\n",
    "                                   'learning_rate': 10e-3,\n",
    "                                   'epochs': 100,\n",
    "                                   'vacant_stall_indicator': False,\n",
    "                                   'bias': True,\n",
    "                                   'model_architecture': LinSig},\n",
    "                   'AlunOwen_v2': {'X_columns': [\"age\", \"trainer_sr\", \"daysLTO\", \"position1_1\", \"position1_2\", \"position1_3\", \"position1_4\", \"position2_1\", \"position2_2\", \"position2_3\", \"position2_4\", \"position3_1\", \"position3_2\", \"position3_3\", \"position3_4\", \"entire\", \"gelding\", \"blinkers\", \"visor\", \"cheekpieces\", \"tonguetie\"],\n",
    "                                   'learning_rate': 10e-3,\n",
    "                                   'epochs': 100,\n",
    "                                   'vacant_stall_indicator': False,\n",
    "                                   'bias': True,\n",
    "                                   'model_architecture': LinDropReluLinSoft},\n",
    "                   'AlunOwen_v3': {'X_columns': [\"age\", \"trainer_sr\", \"daysLTO\", \"position1_1\", \"position1_2\", \"position1_3\", \"position1_4\", \"position2_1\", \"position2_2\", \"position2_3\", \"position2_4\", \"position3_1\", \"position3_2\", \"position3_3\", \"position3_4\", \"entire\", \"gelding\", \"blinkers\", \"visor\", \"cheekpieces\", \"tonguetie\"],\n",
    "                                   'learning_rate': 10e-3,\n",
    "                                   'epochs': 100,\n",
    "                                   'vacant_stall_indicator': False,\n",
    "                                   'bias': False,\n",
    "                                   'model_architecture': MLR}\n",
    "                               }\n",
    "\n",
    "model = 'mktprob_MLR'\n",
    "X_columns = model_inventory[model]['X_columns']\n",
    "learning_rate = model_inventory[model]['learning_rate']\n",
    "epochs = model_inventory[model]['epochs']\n",
    "vacant_stall_indicator = model_inventory[model]['vacant_stall_indicator']\n",
    "bias = model_inventory[model]['bias']\n",
    "model_architecture = model_inventory[model]['model_architecture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "\n",
    "y_columns = [\"win\"]\n",
    "\n",
    "train_data_fn = \"data\\\\runners_train.csv\"\n",
    "test_data_fn = \"data\\\\runners_test.csv\"\n",
    "\n",
    "train_data = RacesDataset(train_data_fn, X_columns, y_columns, vacant_stall_indicator=vacant_stall_indicator)\n",
    "test_data = RacesDataset(test_data_fn, X_columns, y_columns, vacant_stall_indicator=vacant_stall_indicator, scalar=train_data.scalar)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"16\" halign=\"left\">mkt_prob</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stall_number</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11504</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.053763</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11505</th>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.053763</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11506</th>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11507</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11508</th>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.230947</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mkt_prob                                                    \\\n",
       "stall_number        1         2         3         4         5         6    \n",
       "race_id                                                                    \n",
       "11504         0.100000  0.038462  0.125000  0.058824  0.142857  0.053763   \n",
       "11505         0.019608  0.029412  0.142857  0.038462  0.400000  0.058824   \n",
       "11506         0.266667  0.038462  0.014925  0.058824  0.181818  0.038462   \n",
       "11507         0.076923  0.058824  0.019608  0.222222  0.444444  0.029412   \n",
       "11508         0.038462  0.038462  0.133333  0.047619  0.230947  0.200000   \n",
       "\n",
       "                                                                               \\\n",
       "stall_number        7         8         9         10        11   12   13   14   \n",
       "race_id                                                                         \n",
       "11504         0.090909  0.125000  0.000000  0.000000  0.000000  0.0  0.0  0.0   \n",
       "11505         0.053763  0.142857  0.029412  0.222222  0.153846  0.0  0.0  0.0   \n",
       "11506         0.111111  0.029412  0.111111  0.307692  0.000000  0.0  0.0  0.0   \n",
       "11507         0.125000  0.153846  0.066667  0.000000  0.000000  0.0  0.0  0.0   \n",
       "11508         0.058824  0.181818  0.250000  0.000000  0.000000  0.0  0.0  0.0   \n",
       "\n",
       "                        \n",
       "stall_number   15   16  \n",
       "race_id                 \n",
       "11504         0.0  0.0  \n",
       "11505         0.0  0.0  \n",
       "11506         0.0  0.0  \n",
       "11507         0.0  0.0  \n",
       "11508         0.0  0.0  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = 1000 # was 20\n",
    "train_data.races.iloc[:, train_data.X_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: mktprob_MLR\n",
      "Layer: weights | Size: torch.Size([1]) | Values : Parameter containing:\n",
      "tensor([-1.6077], requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build the neural network\n",
    "\n",
    "output_layer_nodes = train_data.y.shape[1]\n",
    "input_layer_nodes = train_data.X.shape[1]\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "net = model_architecture(input_layer_nodes, output_layer_nodes, bias=bias).to(device) # linear-relu-linear-softwax nn (1 hidden layer)\n",
    "print(f\"Model structure: {model}\")\n",
    "\n",
    "for name, param in net.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([11])\n",
      "torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "# example to show how model is used from prediction\n",
    "\n",
    "X = torch.rand(1, input_layer_nodes, device=device)\n",
    "logits = net(X)\n",
    "y_pred = logits.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NOTEBOOK_NAME='C:\\Users\\gille\\OneDrive\\1-Projects\\_Horse Racing 2H22\\New Framework\\3b_Deep Learning.ipynb'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:90fyt58e) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lemon-feather-140</strong> at: <a href='https://wandb.ai/gillenpj/horse-racing-project/runs/90fyt58e' target=\"_blank\">https://wandb.ai/gillenpj/horse-racing-project/runs/90fyt58e</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230425_142241-90fyt58e\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:90fyt58e). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gille\\OneDrive\\1-Projects\\_Horse Racing 2H22\\New Framework\\wandb\\run-20230425_143915-k6ydom6t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gillenpj/horse-racing-project/runs/k6ydom6t' target=\"_blank\">wise-capybara-141</a></strong> to <a href='https://wandb.ai/gillenpj/horse-racing-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gillenpj/horse-racing-project' target=\"_blank\">https://wandb.ai/gillenpj/horse-racing-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gillenpj/horse-racing-project/runs/k6ydom6t' target=\"_blank\">https://wandb.ai/gillenpj/horse-racing-project/runs/k6ydom6t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/gillenpj/horse-racing-project/runs/k6ydom6t?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x14103d70e10>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%env WANDB_NOTEBOOK_NAME 'C:\\Users\\gille\\OneDrive\\1-Projects\\_Horse Racing 2H22\\New Framework\\3b_Deep Learning.ipynb'\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"horse-racing-project\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"device\": device,\n",
    "    \"model\": model,\n",
    "    \"X_columns\": X_columns,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"epochs\": epochs,\n",
    "    \"vacant_stall_indicator\": vacant_stall_indicator,\n",
    "    \"bias\": bias,\n",
    "    \"model_architecture\": list(net.modules())\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.809737  [   64/16620]\n",
      "loss: 2.678330  [ 6464/16620]\n",
      "loss: 2.559772  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.531635 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.655037  [   64/16620]\n",
      "loss: 2.580183  [ 6464/16620]\n",
      "loss: 2.557407  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.529647 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.654825  [   64/16620]\n",
      "loss: 2.578974  [ 6464/16620]\n",
      "loss: 2.557304  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.529152 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.654860  [   64/16620]\n",
      "loss: 2.578566  [ 6464/16620]\n",
      "loss: 2.557343  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528930 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.654915  [   64/16620]\n",
      "loss: 2.578371  [ 6464/16620]\n",
      "loss: 2.557398  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528805 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.654966  [   64/16620]\n",
      "loss: 2.578260  [ 6464/16620]\n",
      "loss: 2.557448  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528726 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.655011  [   64/16620]\n",
      "loss: 2.578192  [ 6464/16620]\n",
      "loss: 2.557491  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528672 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.655048  [   64/16620]\n",
      "loss: 2.578146  [ 6464/16620]\n",
      "loss: 2.557526  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528633 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.655080  [   64/16620]\n",
      "loss: 2.578115  [ 6464/16620]\n",
      "loss: 2.557556  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528605 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.655107  [   64/16620]\n",
      "loss: 2.578092  [ 6464/16620]\n",
      "loss: 2.557581  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528582 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 2.655130  [   64/16620]\n",
      "loss: 2.578076  [ 6464/16620]\n",
      "loss: 2.557602  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528565 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 2.655150  [   64/16620]\n",
      "loss: 2.578063  [ 6464/16620]\n",
      "loss: 2.557620  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528551 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 2.655166  [   64/16620]\n",
      "loss: 2.578053  [ 6464/16620]\n",
      "loss: 2.557635  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528540 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 2.655181  [   64/16620]\n",
      "loss: 2.578046  [ 6464/16620]\n",
      "loss: 2.557647  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528531 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.655193  [   64/16620]\n",
      "loss: 2.578040  [ 6464/16620]\n",
      "loss: 2.557658  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528523 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 2.655203  [   64/16620]\n",
      "loss: 2.578035  [ 6464/16620]\n",
      "loss: 2.557668  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528517 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.655213  [   64/16620]\n",
      "loss: 2.578031  [ 6464/16620]\n",
      "loss: 2.557676  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528512 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.655220  [   64/16620]\n",
      "loss: 2.578028  [ 6464/16620]\n",
      "loss: 2.557682  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528507 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.655227  [   64/16620]\n",
      "loss: 2.578025  [ 6464/16620]\n",
      "loss: 2.557688  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528504 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.655233  [   64/16620]\n",
      "loss: 2.578023  [ 6464/16620]\n",
      "loss: 2.557693  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528500 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.655238  [   64/16620]\n",
      "loss: 2.578021  [ 6464/16620]\n",
      "loss: 2.557698  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528498 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 2.655243  [   64/16620]\n",
      "loss: 2.578020  [ 6464/16620]\n",
      "loss: 2.557702  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528495 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 2.655247  [   64/16620]\n",
      "loss: 2.578018  [ 6464/16620]\n",
      "loss: 2.557705  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528493 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.655250  [   64/16620]\n",
      "loss: 2.578017  [ 6464/16620]\n",
      "loss: 2.557708  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528491 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 2.655253  [   64/16620]\n",
      "loss: 2.578016  [ 6464/16620]\n",
      "loss: 2.557710  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528490 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.655256  [   64/16620]\n",
      "loss: 2.578016  [ 6464/16620]\n",
      "loss: 2.557713  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528489 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 2.655258  [   64/16620]\n",
      "loss: 2.578015  [ 6464/16620]\n",
      "loss: 2.557715  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528487 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.655260  [   64/16620]\n",
      "loss: 2.578014  [ 6464/16620]\n",
      "loss: 2.557716  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528487 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 2.655262  [   64/16620]\n",
      "loss: 2.578014  [ 6464/16620]\n",
      "loss: 2.557718  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528486 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 2.655263  [   64/16620]\n",
      "loss: 2.578013  [ 6464/16620]\n",
      "loss: 2.557719  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528485 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 2.655264  [   64/16620]\n",
      "loss: 2.578013  [ 6464/16620]\n",
      "loss: 2.557720  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528484 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 2.655266  [   64/16620]\n",
      "loss: 2.578013  [ 6464/16620]\n",
      "loss: 2.557721  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528484 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 2.655267  [   64/16620]\n",
      "loss: 2.578012  [ 6464/16620]\n",
      "loss: 2.557722  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528483 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 2.655267  [   64/16620]\n",
      "loss: 2.578012  [ 6464/16620]\n",
      "loss: 2.557723  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528483 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 2.655268  [   64/16620]\n",
      "loss: 2.578012  [ 6464/16620]\n",
      "loss: 2.557723  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528482 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 2.655269  [   64/16620]\n",
      "loss: 2.578012  [ 6464/16620]\n",
      "loss: 2.557724  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528482 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 2.655269  [   64/16620]\n",
      "loss: 2.578012  [ 6464/16620]\n",
      "loss: 2.557724  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528482 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 2.655270  [   64/16620]\n",
      "loss: 2.578011  [ 6464/16620]\n",
      "loss: 2.557725  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528482 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 2.655270  [   64/16620]\n",
      "loss: 2.578011  [ 6464/16620]\n",
      "loss: 2.557725  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528481 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 2.655271  [   64/16620]\n",
      "loss: 2.578011  [ 6464/16620]\n",
      "loss: 2.557725  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528481 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 2.655271  [   64/16620]\n",
      "loss: 2.578011  [ 6464/16620]\n",
      "loss: 2.557726  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528481 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 2.655271  [   64/16620]\n",
      "loss: 2.578011  [ 6464/16620]\n",
      "loss: 2.557726  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528481 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 2.655272  [   64/16620]\n",
      "loss: 2.578011  [ 6464/16620]\n",
      "loss: 2.557726  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528481 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 2.655272  [   64/16620]\n",
      "loss: 2.578011  [ 6464/16620]\n",
      "loss: 2.557726  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528481 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 2.655272  [   64/16620]\n",
      "loss: 2.578011  [ 6464/16620]\n",
      "loss: 2.557726  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528481 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 2.655272  [   64/16620]\n",
      "loss: 2.578011  [ 6464/16620]\n",
      "loss: 2.557727  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528480 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 2.655272  [   64/16620]\n",
      "loss: 2.578011  [ 6464/16620]\n",
      "loss: 2.557727  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528480 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 2.655273  [   64/16620]\n",
      "loss: 2.578011  [ 6464/16620]\n",
      "loss: 2.557727  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528480 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 2.655273  [   64/16620]\n",
      "loss: 2.578011  [ 6464/16620]\n",
      "loss: 2.557727  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528480 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 2.655273  [   64/16620]\n",
      "loss: 2.578011  [ 6464/16620]\n",
      "loss: 2.557727  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.528480 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# optimizing model parameters\n",
    "\n",
    "# initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, net, loss_fn, optimizer, device)\n",
    "    (acc, loss) = test_loop(test_dataloader, net, loss_fn, device)\n",
    "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a4af21bd054e03b5839ca2319aab7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.35149</td></tr><tr><td>loss</td><td>2.52848</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wise-capybara-141</strong> at: <a href='https://wandb.ai/gillenpj/horse-racing-project/runs/k6ydom6t' target=\"_blank\">https://wandb.ai/gillenpj/horse-racing-project/runs/k6ydom6t</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230425_143915-k6ydom6t\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "for para_name, para_vals in net.named_parameters():\n",
    "    np.savetxt(para_name + \".csv\", para_vals.data.numpy(), fmt='%6.3f', delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4ed282fe5a96d451181bcb846a73bf3735bfa0d466ee57d09e22e537eb2d1df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
