{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This idea of this notebook is to create a first (simple) DL model using all the same features used in the multinomial logit model.\n",
    "\n",
    "Much inspiration was derived from https://towardsdatascience.com/use-machine-learning-to-predict-horse-racing-4f1111fb6ced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from importlib import reload\n",
    "import deeplearninglib\n",
    "reload(deeplearninglib)\n",
    "from deeplearninglib import *\n",
    "\n",
    "import wandb\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "\n",
    "# select the same features used in the multinomial model\n",
    "# X_columns = [\"age\", \"sire_sr\", \"dam_sr\", \"trainer_sr\", \"daysLTO\", \"position1_1\", \"position1_2\", \"position1_3\", \"position1_4\", \"position2_1\", \"position2_2\", \"position2_3\", \"position2_4\", \"position3_1\", \"position3_2\", \"position3_3\", \"position3_4\", \"entire\", \"gelding\", \"blinkers\", \"visor\", \"cheekpieces\", \"tonguetie\"]\n",
    "X_columns = [\"mkt_prob\"]\n",
    "y_columns = [\"win\"]\n",
    "\n",
    "train_data = RacesDataset('data\\\\runners_train.csv', X_columns, y_columns)\n",
    "test_data = RacesDataset('data\\\\runners_test.csv', X_columns, y_columns)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"16\" halign=\"left\">mkt_prob</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stall_number</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11504</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.053763</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11505</th>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.053763</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11506</th>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11507</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11508</th>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.230947</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mkt_prob                                                    \\\n",
       "stall_number        1         2         3         4         5         6    \n",
       "race_id                                                                    \n",
       "11504         0.100000  0.038462  0.125000  0.058824  0.142857  0.053763   \n",
       "11505         0.019608  0.029412  0.142857  0.038462  0.400000  0.058824   \n",
       "11506         0.266667  0.038462  0.014925  0.058824  0.181818  0.038462   \n",
       "11507         0.076923  0.058824  0.019608  0.222222  0.444444  0.029412   \n",
       "11508         0.038462  0.038462  0.133333  0.047619  0.230947  0.200000   \n",
       "\n",
       "                                                                               \\\n",
       "stall_number        7         8         9         10        11   12   13   14   \n",
       "race_id                                                                         \n",
       "11504         0.090909  0.125000  0.000000  0.000000  0.000000  0.0  0.0  0.0   \n",
       "11505         0.053763  0.142857  0.029412  0.222222  0.153846  0.0  0.0  0.0   \n",
       "11506         0.111111  0.029412  0.111111  0.307692  0.000000  0.0  0.0  0.0   \n",
       "11507         0.125000  0.153846  0.066667  0.000000  0.000000  0.0  0.0  0.0   \n",
       "11508         0.058824  0.181818  0.250000  0.000000  0.000000  0.0  0.0  0.0   \n",
       "\n",
       "                        \n",
       "stall_number   15   16  \n",
       "race_id                 \n",
       "11504         0.0  0.0  \n",
       "11505         0.0  0.0  \n",
       "11506         0.0  0.0  \n",
       "11507         0.0  0.0  \n",
       "11508         0.0  0.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = 1000 # was 20\n",
    "train_data.races.iloc[:, train_data.X_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: LRLS(\n",
      "  (neural_network): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=False)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=16, out_features=16, bias=False)\n",
      "    (3): Softmax(dim=1)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "Layer: neural_network.0.weight | Size: torch.Size([16, 16]) | Values : Parameter containing:\n",
      "tensor([[-0.0019,  0.1341, -0.2058, -0.1840, -0.0963,  0.0670, -0.0050,  0.1982,\n",
      "         -0.0222,  0.0662, -0.0756, -0.0491, -0.2388, -0.1656, -0.1031,  0.0093],\n",
      "        [ 0.0988,  0.1500, -0.1695, -0.1089,  0.0908,  0.2076, -0.0515,  0.1871,\n",
      "         -0.0403,  0.0265,  0.2264, -0.2319, -0.1574, -0.0633, -0.0974,  0.2160],\n",
      "        [-0.1620, -0.1151, -0.1747, -0.2341, -0.1459,  0.2149,  0.1116,  0.1212,\n",
      "          0.0131, -0.1282,  0.0423, -0.2334, -0.1806, -0.1289,  0.1577,  0.1466],\n",
      "        [-0.1109, -0.0090,  0.1599,  0.2485,  0.0992,  0.0338,  0.1676, -0.1472,\n",
      "          0.0466, -0.1938, -0.1733, -0.1291,  0.1131,  0.1005, -0.1481,  0.0755],\n",
      "        [ 0.1372, -0.0316,  0.0095,  0.0579,  0.1551,  0.2400, -0.1927, -0.0916,\n",
      "          0.0983,  0.2071,  0.2176,  0.2206,  0.0498, -0.2174,  0.0230, -0.1564],\n",
      "        [-0.2330,  0.2221,  0.1901, -0.2494,  0.0468, -0.0421, -0.0411, -0.1144,\n",
      "          0.0961, -0.1481,  0.0916,  0.1264,  0.1790,  0.0935, -0.2474, -0.1622],\n",
      "        [ 0.1248,  0.0523, -0.1950, -0.1440,  0.2352,  0.1685, -0.1090, -0.0629,\n",
      "         -0.2381, -0.0045, -0.1883, -0.1928, -0.0138,  0.0375, -0.1024,  0.1483],\n",
      "        [-0.1521,  0.2268,  0.1713, -0.2108, -0.0622,  0.0113,  0.0365,  0.0593,\n",
      "          0.0981,  0.0150, -0.1220,  0.1183, -0.2398, -0.1482, -0.0626, -0.1218],\n",
      "        [-0.0875, -0.2049, -0.0532,  0.0534, -0.1629, -0.0128,  0.1790, -0.0257,\n",
      "          0.0069, -0.0216,  0.0506,  0.1590,  0.2368,  0.1588,  0.2374, -0.0181],\n",
      "        [-0.2246, -0.1185,  0.1702, -0.0016, -0.1243, -0.1916, -0.2340, -0.2110,\n",
      "         -0.0507,  0.1371,  0.1352, -0.2411,  0.1559, -0.1956, -0.0529, -0.1014],\n",
      "        [-0.0482, -0.0491, -0.2243, -0.2159, -0.0391,  0.0032, -0.1136,  0.0942,\n",
      "         -0.2250, -0.0169,  0.2199, -0.1020,  0.2258,  0.0905, -0.2256,  0.1582],\n",
      "        [-0.0288, -0.1116,  0.1999, -0.2020,  0.0268, -0.0523,  0.1785,  0.0698,\n",
      "          0.1201,  0.0883, -0.0601, -0.0526, -0.2060,  0.1355,  0.1985,  0.1711],\n",
      "        [-0.1763,  0.0111, -0.1762, -0.1376, -0.1457,  0.0854, -0.1490, -0.0055,\n",
      "          0.0105,  0.1612, -0.1890, -0.1716, -0.1452,  0.1750, -0.0899,  0.2109],\n",
      "        [ 0.0904,  0.0317, -0.0019, -0.0494,  0.0314, -0.0571, -0.0018,  0.0319,\n",
      "         -0.1956, -0.1310,  0.2019, -0.2029, -0.0180,  0.2473,  0.0903,  0.0071],\n",
      "        [-0.2167,  0.1238, -0.1781, -0.0710, -0.0839, -0.0370,  0.0027,  0.2062,\n",
      "          0.0312,  0.2239,  0.1529, -0.1581,  0.1121, -0.1767, -0.1060,  0.0735],\n",
      "        [ 0.0825,  0.1876, -0.0805,  0.0004,  0.1287, -0.2418,  0.1807, -0.2067,\n",
      "          0.0034, -0.0425, -0.1317,  0.0330,  0.2067, -0.0731, -0.1484, -0.0925]],\n",
      "       requires_grad=True) \n",
      "\n",
      "Layer: neural_network.2.weight | Size: torch.Size([16, 16]) | Values : Parameter containing:\n",
      "tensor([[-0.2478,  0.1128, -0.1201, -0.1668, -0.1440,  0.1437,  0.1324,  0.1919,\n",
      "          0.0907, -0.0835, -0.0699,  0.0739,  0.2055,  0.0680, -0.1183, -0.1175],\n",
      "        [-0.2364,  0.0540, -0.1403, -0.2229,  0.2192, -0.1624, -0.0284,  0.0716,\n",
      "          0.0080, -0.1682, -0.2021,  0.1993,  0.0407,  0.2074, -0.0838,  0.0736],\n",
      "        [-0.0572, -0.0111, -0.1523,  0.0846,  0.0790, -0.0052, -0.0562, -0.1541,\n",
      "          0.1729, -0.1861,  0.1024, -0.0841, -0.1206,  0.0449, -0.1299,  0.0576],\n",
      "        [ 0.0491, -0.1856,  0.0416,  0.1065,  0.0990, -0.0315, -0.2049, -0.0385,\n",
      "          0.0868, -0.0912,  0.0949,  0.1665, -0.1305,  0.0025,  0.1034,  0.0196],\n",
      "        [ 0.0209,  0.0312, -0.1965,  0.0196,  0.1731,  0.2253,  0.1469,  0.0335,\n",
      "          0.1168, -0.1216, -0.2072, -0.2150,  0.2494,  0.1587, -0.1728,  0.0978],\n",
      "        [ 0.1888,  0.2499,  0.2186,  0.1937, -0.0573, -0.0877,  0.2053,  0.1401,\n",
      "         -0.1504,  0.2248,  0.1208,  0.1363, -0.1567,  0.0717, -0.0876,  0.1953],\n",
      "        [-0.0450,  0.0973,  0.0444,  0.1064, -0.0850,  0.1219, -0.1746,  0.0565,\n",
      "         -0.1691, -0.2466, -0.2008,  0.1974,  0.1353,  0.2346,  0.2003, -0.2233],\n",
      "        [-0.1706, -0.0404, -0.1624,  0.1736, -0.1890, -0.1220, -0.2415, -0.1419,\n",
      "          0.2056,  0.2047,  0.1790,  0.1930,  0.2223, -0.0640,  0.1100,  0.2227],\n",
      "        [ 0.0827,  0.2499,  0.1297,  0.1554, -0.0875,  0.1200,  0.0287, -0.0597,\n",
      "         -0.1409, -0.1403, -0.1924,  0.1678,  0.1777, -0.0285, -0.1447,  0.1932],\n",
      "        [ 0.1599,  0.0186, -0.1180,  0.2298,  0.1022, -0.1898,  0.2393,  0.1898,\n",
      "         -0.0911,  0.1405, -0.1420, -0.0392,  0.2123,  0.0103, -0.1768, -0.0836],\n",
      "        [-0.0679, -0.0482,  0.0239,  0.2312,  0.0134, -0.1544,  0.0128,  0.1199,\n",
      "          0.1240, -0.2285, -0.0447, -0.1858, -0.1067,  0.0901, -0.1775,  0.0929],\n",
      "        [ 0.2122,  0.0164, -0.1666, -0.0896,  0.0546, -0.1906,  0.1242, -0.2270,\n",
      "         -0.2403, -0.2429, -0.0507,  0.1681, -0.2366,  0.2078, -0.1000,  0.0732],\n",
      "        [ 0.0114, -0.2254,  0.2073,  0.1346,  0.2485,  0.1263, -0.1650,  0.2086,\n",
      "          0.0134,  0.1186, -0.2005, -0.0719, -0.2455, -0.0974,  0.0539, -0.1963],\n",
      "        [ 0.0797,  0.1342,  0.0348, -0.1673, -0.1938, -0.0771,  0.1097,  0.2466,\n",
      "          0.1438, -0.0282,  0.0877, -0.2453, -0.2135,  0.1167, -0.1416,  0.1203],\n",
      "        [-0.1765, -0.1238, -0.2059,  0.1305, -0.0255,  0.1924,  0.1547,  0.1383,\n",
      "          0.0080, -0.0773, -0.0544,  0.0332,  0.1239, -0.1751,  0.2098, -0.0272],\n",
      "        [-0.2095, -0.1353,  0.2212,  0.2286, -0.2316,  0.1763,  0.1253,  0.1480,\n",
      "          0.2116, -0.1347,  0.0789,  0.1023, -0.0739,  0.0837, -0.0719,  0.1546]],\n",
      "       requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build the neural network\n",
    "\n",
    "output_layer_nodes = train_data.y.shape[1]\n",
    "input_layer_nodes = train_data.X.shape[1]\n",
    "\n",
    "torch.manual_seed(0)\n",
    "model = LRLS(input_layer_nodes, output_layer_nodes).to(device) # linear-relu-linear-softwax neural net with 1 hidden layer\n",
    "print(f\"Model structure: {model}\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([1])\n",
      "torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "# example to show how model is used from prediction\n",
    "\n",
    "X = torch.rand(1, input_layer_nodes, device=device)\n",
    "logits = model(X)\n",
    "# pred_probab = nn.Softmax(dim=1)(logits)\n",
    "# y_pred = pred_probab.argmax(1)\n",
    "y_pred = logits.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: \"WANDB_NOTEBOOK_NAME\"=\"3b_Deep Learning.ipynb\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gille\\OneDrive\\1-Projects\\_Horse Racing 2H22\\New Framework\\wandb\\run-20230420_140035-48t6fwhp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gillenpj/horse-racing-project/runs/48t6fwhp' target=\"_blank\">mild-universe-31</a></strong> to <a href='https://wandb.ai/gillenpj/horse-racing-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gillenpj/horse-racing-project' target=\"_blank\">https://wandb.ai/gillenpj/horse-racing-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gillenpj/horse-racing-project/runs/48t6fwhp' target=\"_blank\">https://wandb.ai/gillenpj/horse-racing-project/runs/48t6fwhp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.772529  [   64/16620]\n",
      "loss: 2.771539  [ 6464/16620]\n",
      "loss: 2.814276  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 12.5%, Avg loss: 2.777272 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.770969  [   64/16620]\n",
      "loss: 2.766967  [ 6464/16620]\n",
      "loss: 2.806212  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 11.8%, Avg loss: 2.766496 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.763449  [   64/16620]\n",
      "loss: 2.743095  [ 6464/16620]\n",
      "loss: 2.773904  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 10.8%, Avg loss: 2.738477 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.741557  [   64/16620]\n",
      "loss: 2.699145  [ 6464/16620]\n",
      "loss: 2.738662  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 18.2%, Avg loss: 2.714659 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.714838  [   64/16620]\n",
      "loss: 2.665002  [ 6464/16620]\n",
      "loss: 2.680048  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 24.0%, Avg loss: 2.669797 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.670493  [   64/16620]\n",
      "loss: 2.617899  [ 6464/16620]\n",
      "loss: 2.626883  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 25.0%, Avg loss: 2.640450 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.645020  [   64/16620]\n",
      "loss: 2.586237  [ 6464/16620]\n",
      "loss: 2.607200  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 25.3%, Avg loss: 2.630688 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.635954  [   64/16620]\n",
      "loss: 2.575243  [ 6464/16620]\n",
      "loss: 2.599432  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 25.3%, Avg loss: 2.626522 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.632026  [   64/16620]\n",
      "loss: 2.572203  [ 6464/16620]\n",
      "loss: 2.594744  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 25.3%, Avg loss: 2.623112 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.630089  [   64/16620]\n",
      "loss: 2.574105  [ 6464/16620]\n",
      "loss: 2.591209  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 26.9%, Avg loss: 2.614989 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 2.629520  [   64/16620]\n",
      "loss: 2.585641  [ 6464/16620]\n",
      "loss: 2.587676  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 28.0%, Avg loss: 2.604562 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 2.624292  [   64/16620]\n",
      "loss: 2.591477  [ 6464/16620]\n",
      "loss: 2.586934  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 28.1%, Avg loss: 2.599433 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 2.618683  [   64/16620]\n",
      "loss: 2.591875  [ 6464/16620]\n",
      "loss: 2.587017  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 28.4%, Avg loss: 2.597113 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 2.615122  [   64/16620]\n",
      "loss: 2.591769  [ 6464/16620]\n",
      "loss: 2.587130  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 28.3%, Avg loss: 2.595866 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.612474  [   64/16620]\n",
      "loss: 2.591918  [ 6464/16620]\n",
      "loss: 2.587068  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 28.3%, Avg loss: 2.595080 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 2.610444  [   64/16620]\n",
      "loss: 2.592317  [ 6464/16620]\n",
      "loss: 2.586701  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 28.3%, Avg loss: 2.594472 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.608982  [   64/16620]\n",
      "loss: 2.592705  [ 6464/16620]\n",
      "loss: 2.585919  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 28.4%, Avg loss: 2.593888 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.607953  [   64/16620]\n",
      "loss: 2.592830  [ 6464/16620]\n",
      "loss: 2.584484  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 28.4%, Avg loss: 2.593128 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.607166  [   64/16620]\n",
      "loss: 2.592234  [ 6464/16620]\n",
      "loss: 2.581519  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 28.5%, Avg loss: 2.591633 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.606311  [   64/16620]\n",
      "loss: 2.589064  [ 6464/16620]\n",
      "loss: 2.573755  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 29.5%, Avg loss: 2.587522 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.604745  [   64/16620]\n",
      "loss: 2.580306  [ 6464/16620]\n",
      "loss: 2.562236  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 29.9%, Avg loss: 2.582523 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 2.602997  [   64/16620]\n",
      "loss: 2.575078  [ 6464/16620]\n",
      "loss: 2.554579  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.2%, Avg loss: 2.579355 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 2.602364  [   64/16620]\n",
      "loss: 2.572496  [ 6464/16620]\n",
      "loss: 2.550379  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.3%, Avg loss: 2.577444 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.602313  [   64/16620]\n",
      "loss: 2.570547  [ 6464/16620]\n",
      "loss: 2.548013  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.5%, Avg loss: 2.576251 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 2.602611  [   64/16620]\n",
      "loss: 2.568642  [ 6464/16620]\n",
      "loss: 2.546435  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.5%, Avg loss: 2.575503 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.603160  [   64/16620]\n",
      "loss: 2.566782  [ 6464/16620]\n",
      "loss: 2.545395  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.5%, Avg loss: 2.575005 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 2.603875  [   64/16620]\n",
      "loss: 2.565258  [ 6464/16620]\n",
      "loss: 2.544690  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.574653 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.604687  [   64/16620]\n",
      "loss: 2.563702  [ 6464/16620]\n",
      "loss: 2.544162  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.574389 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 2.605548  [   64/16620]\n",
      "loss: 2.562367  [ 6464/16620]\n",
      "loss: 2.543743  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.574185 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 2.606351  [   64/16620]\n",
      "loss: 2.561067  [ 6464/16620]\n",
      "loss: 2.543348  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.574031 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 2.607113  [   64/16620]\n",
      "loss: 2.560156  [ 6464/16620]\n",
      "loss: 2.543023  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.5%, Avg loss: 2.573911 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 2.607720  [   64/16620]\n",
      "loss: 2.559393  [ 6464/16620]\n",
      "loss: 2.542728  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.573813 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 2.608332  [   64/16620]\n",
      "loss: 2.558745  [ 6464/16620]\n",
      "loss: 2.542466  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.573732 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 2.608723  [   64/16620]\n",
      "loss: 2.558207  [ 6464/16620]\n",
      "loss: 2.542144  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.573670 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 2.609074  [   64/16620]\n",
      "loss: 2.557764  [ 6464/16620]\n",
      "loss: 2.541844  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.573618 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 2.609440  [   64/16620]\n",
      "loss: 2.557405  [ 6464/16620]\n",
      "loss: 2.541558  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.573577 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 2.609721  [   64/16620]\n",
      "loss: 2.557068  [ 6464/16620]\n",
      "loss: 2.541248  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.573542 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 2.609980  [   64/16620]\n",
      "loss: 2.556816  [ 6464/16620]\n",
      "loss: 2.540897  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.573517 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 2.610273  [   64/16620]\n",
      "loss: 2.556615  [ 6464/16620]\n",
      "loss: 2.540523  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.573492 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 2.610523  [   64/16620]\n",
      "loss: 2.556508  [ 6464/16620]\n",
      "loss: 2.540130  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.573461 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 2.610791  [   64/16620]\n",
      "loss: 2.556354  [ 6464/16620]\n",
      "loss: 2.539723  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.573427 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 2.611036  [   64/16620]\n",
      "loss: 2.556201  [ 6464/16620]\n",
      "loss: 2.539325  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.573391 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 2.611289  [   64/16620]\n",
      "loss: 2.556125  [ 6464/16620]\n",
      "loss: 2.538980  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.573354 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 2.611549  [   64/16620]\n",
      "loss: 2.556119  [ 6464/16620]\n",
      "loss: 2.538651  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.573300 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 2.611897  [   64/16620]\n",
      "loss: 2.556230  [ 6464/16620]\n",
      "loss: 2.538340  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.573231 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 2.612340  [   64/16620]\n",
      "loss: 2.556398  [ 6464/16620]\n",
      "loss: 2.538034  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.573139 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 2.612721  [   64/16620]\n",
      "loss: 2.556515  [ 6464/16620]\n",
      "loss: 2.537851  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.573011 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 2.612977  [   64/16620]\n",
      "loss: 2.556879  [ 6464/16620]\n",
      "loss: 2.537659  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.572812 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 2.613492  [   64/16620]\n",
      "loss: 2.557349  [ 6464/16620]\n",
      "loss: 2.537345  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.4%, Avg loss: 2.572479 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 2.614190  [   64/16620]\n",
      "loss: 2.558207  [ 6464/16620]\n",
      "loss: 2.536632  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.6%, Avg loss: 2.571817 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 2.615368  [   64/16620]\n",
      "loss: 2.559621  [ 6464/16620]\n",
      "loss: 2.535052  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.8%, Avg loss: 2.570487 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 2.617242  [   64/16620]\n",
      "loss: 2.561640  [ 6464/16620]\n",
      "loss: 2.531637  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.0%, Avg loss: 2.568359 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 2.619345  [   64/16620]\n",
      "loss: 2.563193  [ 6464/16620]\n",
      "loss: 2.528547  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.2%, Avg loss: 2.565961 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 2.620732  [   64/16620]\n",
      "loss: 2.564102  [ 6464/16620]\n",
      "loss: 2.527172  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.5%, Avg loss: 2.563691 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 2.622107  [   64/16620]\n",
      "loss: 2.563546  [ 6464/16620]\n",
      "loss: 2.526812  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.8%, Avg loss: 2.561945 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 2.623258  [   64/16620]\n",
      "loss: 2.562524  [ 6464/16620]\n",
      "loss: 2.526523  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.8%, Avg loss: 2.560745 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 2.624591  [   64/16620]\n",
      "loss: 2.561741  [ 6464/16620]\n",
      "loss: 2.526225  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.559957 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 2.625785  [   64/16620]\n",
      "loss: 2.561390  [ 6464/16620]\n",
      "loss: 2.525952  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.559445 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 2.626874  [   64/16620]\n",
      "loss: 2.561174  [ 6464/16620]\n",
      "loss: 2.525781  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.559102 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 2.627904  [   64/16620]\n",
      "loss: 2.560883  [ 6464/16620]\n",
      "loss: 2.525606  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558876 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 2.628900  [   64/16620]\n",
      "loss: 2.560524  [ 6464/16620]\n",
      "loss: 2.525409  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558718 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 2.629939  [   64/16620]\n",
      "loss: 2.560051  [ 6464/16620]\n",
      "loss: 2.525232  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558589 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 2.631003  [   64/16620]\n",
      "loss: 2.559650  [ 6464/16620]\n",
      "loss: 2.525052  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.8%, Avg loss: 2.558485 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 2.631916  [   64/16620]\n",
      "loss: 2.559064  [ 6464/16620]\n",
      "loss: 2.524881  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558399 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 2.632737  [   64/16620]\n",
      "loss: 2.558675  [ 6464/16620]\n",
      "loss: 2.524707  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558316 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 2.633480  [   64/16620]\n",
      "loss: 2.558144  [ 6464/16620]\n",
      "loss: 2.524647  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.558243 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 2.634045  [   64/16620]\n",
      "loss: 2.557584  [ 6464/16620]\n",
      "loss: 2.524588  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.558183 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 2.634510  [   64/16620]\n",
      "loss: 2.556975  [ 6464/16620]\n",
      "loss: 2.524590  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.558131 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 2.634814  [   64/16620]\n",
      "loss: 2.556482  [ 6464/16620]\n",
      "loss: 2.524630  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.558092 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 2.635048  [   64/16620]\n",
      "loss: 2.555958  [ 6464/16620]\n",
      "loss: 2.524681  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 32.1%, Avg loss: 2.558068 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 2.635190  [   64/16620]\n",
      "loss: 2.555516  [ 6464/16620]\n",
      "loss: 2.524816  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.558045 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 2.635231  [   64/16620]\n",
      "loss: 2.555113  [ 6464/16620]\n",
      "loss: 2.524983  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.558040 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 2.635132  [   64/16620]\n",
      "loss: 2.554850  [ 6464/16620]\n",
      "loss: 2.525148  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.558020 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 2.635066  [   64/16620]\n",
      "loss: 2.554497  [ 6464/16620]\n",
      "loss: 2.525241  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.558017 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 2.634893  [   64/16620]\n",
      "loss: 2.554201  [ 6464/16620]\n",
      "loss: 2.525321  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.558021 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 2.634744  [   64/16620]\n",
      "loss: 2.553900  [ 6464/16620]\n",
      "loss: 2.525423  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.558028 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 2.634586  [   64/16620]\n",
      "loss: 2.553666  [ 6464/16620]\n",
      "loss: 2.525609  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.558036 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 2.634419  [   64/16620]\n",
      "loss: 2.553450  [ 6464/16620]\n",
      "loss: 2.525750  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558042 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 2.634270  [   64/16620]\n",
      "loss: 2.553075  [ 6464/16620]\n",
      "loss: 2.525918  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558043 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 2.634108  [   64/16620]\n",
      "loss: 2.552823  [ 6464/16620]\n",
      "loss: 2.526019  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558055 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 2.633852  [   64/16620]\n",
      "loss: 2.552580  [ 6464/16620]\n",
      "loss: 2.526128  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558061 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 2.633661  [   64/16620]\n",
      "loss: 2.552297  [ 6464/16620]\n",
      "loss: 2.526257  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558071 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 2.633425  [   64/16620]\n",
      "loss: 2.552081  [ 6464/16620]\n",
      "loss: 2.526318  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558078 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 2.633162  [   64/16620]\n",
      "loss: 2.551830  [ 6464/16620]\n",
      "loss: 2.526434  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558085 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 2.632867  [   64/16620]\n",
      "loss: 2.551627  [ 6464/16620]\n",
      "loss: 2.526591  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.558094 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 2.632608  [   64/16620]\n",
      "loss: 2.551473  [ 6464/16620]\n",
      "loss: 2.526774  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.558102 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 2.632359  [   64/16620]\n",
      "loss: 2.551325  [ 6464/16620]\n",
      "loss: 2.526958  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558122 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 2.632101  [   64/16620]\n",
      "loss: 2.551188  [ 6464/16620]\n",
      "loss: 2.527167  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558126 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 2.631845  [   64/16620]\n",
      "loss: 2.551001  [ 6464/16620]\n",
      "loss: 2.527435  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558134 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 2.631584  [   64/16620]\n",
      "loss: 2.550878  [ 6464/16620]\n",
      "loss: 2.527663  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558139 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 2.631332  [   64/16620]\n",
      "loss: 2.550739  [ 6464/16620]\n",
      "loss: 2.527956  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558136 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 2.631105  [   64/16620]\n",
      "loss: 2.550595  [ 6464/16620]\n",
      "loss: 2.528154  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558144 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 2.630825  [   64/16620]\n",
      "loss: 2.550450  [ 6464/16620]\n",
      "loss: 2.528340  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558148 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 2.630568  [   64/16620]\n",
      "loss: 2.550349  [ 6464/16620]\n",
      "loss: 2.528578  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558148 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 2.630336  [   64/16620]\n",
      "loss: 2.550317  [ 6464/16620]\n",
      "loss: 2.528837  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558135 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 2.630197  [   64/16620]\n",
      "loss: 2.550258  [ 6464/16620]\n",
      "loss: 2.529063  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558138 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 2.629960  [   64/16620]\n",
      "loss: 2.550256  [ 6464/16620]\n",
      "loss: 2.529325  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.558140 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 2.629646  [   64/16620]\n",
      "loss: 2.550166  [ 6464/16620]\n",
      "loss: 2.529587  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.558127 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 2.629376  [   64/16620]\n",
      "loss: 2.550205  [ 6464/16620]\n",
      "loss: 2.529875  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.558106 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 2.629120  [   64/16620]\n",
      "loss: 2.550304  [ 6464/16620]\n",
      "loss: 2.530110  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 31.9%, Avg loss: 2.558074 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6e129f1dad4cbfb797db5d058d77ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▂▁▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>loss</td><td>█▇▄▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.31937</td></tr><tr><td>loss</td><td>2.55807</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mild-universe-31</strong> at: <a href='https://wandb.ai/gillenpj/horse-racing-project/runs/48t6fwhp' target=\"_blank\">https://wandb.ai/gillenpj/horse-racing-project/runs/48t6fwhp</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230420_140035-48t6fwhp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%env \"WANDB_NOTEBOOK_NAME\" \"3b_Deep Learning.ipynb\"\n",
    "\n",
    "# optimizing model parameters\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"horse-racing-project\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 10e-1,\n",
    "    \"architecture\": \"Simplest\",\n",
    "    \"dataset\": \"full\",\n",
    "    \"epochs\": 100,\n",
    "    \"device\": device\n",
    "    }\n",
    ")\n",
    "\n",
    "# hyperparameters\n",
    "learning_rate = wandb.config.learning_rate\n",
    "epochs = wandb.config.epochs\n",
    "\n",
    "# initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
    "    (acc, loss) = test_loop(test_dataloader, model, loss_fn, device)\n",
    "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "print(\"Done!\")\n",
    "\n",
    "# finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for para_name, para_vals in model.named_parameters():\n",
    "    np.savetxt(para_name + \".csv\", para_vals.data.numpy(), fmt='%6.3f', delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4ed282fe5a96d451181bcb846a73bf3735bfa0d466ee57d09e22e537eb2d1df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
