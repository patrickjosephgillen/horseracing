{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This idea of this notebook is to create a first (simple) DL model using all the same features used in the multinomial logit model.\n",
    "\n",
    "Much inspiration was derived from https://towardsdatascience.com/use-machine-learning-to-predict-horse-racing-4f1111fb6ced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from importlib import reload\n",
    "import deeplearninglib\n",
    "reload(deeplearninglib)\n",
    "from deeplearninglib import *\n",
    "\n",
    "import wandb\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "\n",
    "# select the same features used in the multinomial model\n",
    "# X_columns = [\"age\", \"sire_sr\", \"dam_sr\", \"trainer_sr\", \"daysLTO\", \"position1_1\", \"position1_2\", \"position1_3\", \"position1_4\", \"position2_1\", \"position2_2\", \"position2_3\", \"position2_4\", \"position3_1\", \"position3_2\", \"position3_3\", \"position3_4\", \"entire\", \"gelding\", \"blinkers\", \"visor\", \"cheekpieces\", \"tonguetie\"]\n",
    "# X_columns = [\"sire_sr\", \"dam_sr\", \"trainer_sr\", \"position1_1\", \"position1_2\", \"position1_3\", \"position1_4\", \"position2_1\", \"position2_2\", \"position2_3\", \"position2_4\", \"position3_1\", \"position3_2\", \"position3_3\", \"position3_4\", \"entire\", \"gelding\", \"blinkers\", \"visor\", \"cheekpieces\", \"tonguetie\"]\n",
    "# X_columns = [\"position1_1\", \"position1_2\", \"position1_3\", \"position1_4\", \"position2_1\", \"position2_2\", \"position2_3\", \"position2_4\", \"position3_1\", \"position3_2\", \"position3_3\", \"position3_4\"]\n",
    "X_columns = [\"mkt_prob\"]\n",
    "y_columns = [\"win\"]\n",
    "\n",
    "train_data_fn = \"data\\\\runners_train.csv\"\n",
    "test_data_fn = \"data\\\\runners_test.csv\"\n",
    "\n",
    "train_data = RacesDataset(train_data_fn, X_columns, y_columns)\n",
    "test_data = RacesDataset(test_data_fn, X_columns, y_columns)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"16\" halign=\"left\">mkt_prob</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stall_number</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11504</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.053763</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11505</th>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.053763</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11506</th>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11507</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11508</th>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.230947</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mkt_prob                                                    \\\n",
       "stall_number        1         2         3         4         5         6    \n",
       "race_id                                                                    \n",
       "11504         0.100000  0.038462  0.125000  0.058824  0.142857  0.053763   \n",
       "11505         0.019608  0.029412  0.142857  0.038462  0.400000  0.058824   \n",
       "11506         0.266667  0.038462  0.014925  0.058824  0.181818  0.038462   \n",
       "11507         0.076923  0.058824  0.019608  0.222222  0.444444  0.029412   \n",
       "11508         0.038462  0.038462  0.133333  0.047619  0.230947  0.200000   \n",
       "\n",
       "                                                                               \\\n",
       "stall_number        7         8         9         10        11   12   13   14   \n",
       "race_id                                                                         \n",
       "11504         0.090909  0.125000  0.000000  0.000000  0.000000  0.0  0.0  0.0   \n",
       "11505         0.053763  0.142857  0.029412  0.222222  0.153846  0.0  0.0  0.0   \n",
       "11506         0.111111  0.029412  0.111111  0.307692  0.000000  0.0  0.0  0.0   \n",
       "11507         0.125000  0.153846  0.066667  0.000000  0.000000  0.0  0.0  0.0   \n",
       "11508         0.058824  0.181818  0.250000  0.000000  0.000000  0.0  0.0  0.0   \n",
       "\n",
       "                        \n",
       "stall_number   15   16  \n",
       "race_id                 \n",
       "11504         0.0  0.0  \n",
       "11505         0.0  0.0  \n",
       "11506         0.0  0.0  \n",
       "11507         0.0  0.0  \n",
       "11508         0.0  0.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = 1000 # was 20\n",
    "train_data.races.iloc[:, train_data.X_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure: net(\n",
      "  (neural_network): Sequential(\n",
      "    (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Layer: neural_network.0.weight | Size: torch.Size([16, 16]) | Values : Parameter containing:\n",
      "tensor([[ 0.1277,  0.2205, -0.2487,  0.1354, -0.0726, -0.2448,  0.0555, -0.1361,\n",
      "          0.0354, -0.1623, -0.1881, -0.0212, -0.0840,  0.1462,  0.0216,  0.1951],\n",
      "        [ 0.0469, -0.0804,  0.1693,  0.0137,  0.0469, -0.0804,  0.0009, -0.0481,\n",
      "         -0.0213, -0.1950, -0.0015, -0.2075, -0.0538,  0.1449, -0.1738, -0.1685],\n",
      "        [-0.0610, -0.0685,  0.0423,  0.0415,  0.1268, -0.1722, -0.0176,  0.1398,\n",
      "          0.2162, -0.0182, -0.1447,  0.0719,  0.1424, -0.1562, -0.1451,  0.1105],\n",
      "        [-0.0175, -0.2361, -0.1441,  0.1014, -0.0848,  0.1726,  0.1976,  0.0364,\n",
      "         -0.0198, -0.0794, -0.0126,  0.0455, -0.1910, -0.0597, -0.2080,  0.1534],\n",
      "        [-0.1592,  0.2284, -0.0644, -0.1432,  0.1201,  0.0373,  0.1731,  0.1044,\n",
      "         -0.2408,  0.1581, -0.0471, -0.1105,  0.1588,  0.1823, -0.2197, -0.0226],\n",
      "        [ 0.2053,  0.0968,  0.2106, -0.0857, -0.1379,  0.2150,  0.1042,  0.2400,\n",
      "         -0.1044, -0.1605, -0.0293, -0.2354,  0.0980,  0.1844,  0.0600, -0.0247],\n",
      "        [ 0.1240, -0.1587,  0.2446, -0.2486, -0.2395, -0.0591,  0.2042,  0.0250,\n",
      "          0.0960, -0.1833,  0.0912, -0.0279,  0.1002,  0.1766,  0.1087, -0.0213],\n",
      "        [-0.0154, -0.1568, -0.0904,  0.1625, -0.1002,  0.1553, -0.0991, -0.0582,\n",
      "          0.0053, -0.2294, -0.0025, -0.0252, -0.0225, -0.0500,  0.1971,  0.1845],\n",
      "        [-0.1694,  0.1161, -0.1961, -0.2128,  0.0764,  0.0037, -0.1166, -0.2411,\n",
      "         -0.0968,  0.0835, -0.2314, -0.2429,  0.0317, -0.1801, -0.2190, -0.0963],\n",
      "        [-0.0403, -0.2272, -0.1669,  0.0164, -0.2357, -0.0351, -0.1614, -0.1733,\n",
      "          0.0983, -0.1924, -0.1895,  0.1426,  0.0467,  0.2277, -0.0668, -0.1208],\n",
      "        [-0.0231,  0.0013,  0.1907, -0.2490, -0.0178,  0.1106, -0.1377, -0.1270,\n",
      "          0.1208, -0.2395,  0.0592, -0.1935,  0.1243, -0.2138,  0.1537, -0.2367],\n",
      "        [-0.2101,  0.0037,  0.1970, -0.2141, -0.1115,  0.2196, -0.0419,  0.1036,\n",
      "         -0.2405, -0.0786, -0.1270,  0.2108,  0.1038, -0.1824,  0.0403, -0.0029],\n",
      "        [-0.1027, -0.1428, -0.0830,  0.1971, -0.1792, -0.1781, -0.0771,  0.0043,\n",
      "          0.0681,  0.0075, -0.0602, -0.1516, -0.0449, -0.1006,  0.2020, -0.2434],\n",
      "        [-0.1595, -0.2034, -0.0457,  0.1948,  0.2191, -0.2176, -0.0108,  0.1467,\n",
      "          0.1102,  0.2048, -0.0069, -0.1680,  0.1654, -0.0857,  0.1294, -0.0616],\n",
      "        [ 0.1063,  0.2249,  0.2325, -0.2275, -0.1654, -0.1034, -0.1594,  0.2399,\n",
      "         -0.0221, -0.1658, -0.1112, -0.1381,  0.1020, -0.2181, -0.2331, -0.2464],\n",
      "        [ 0.1076,  0.1236, -0.1553,  0.0620,  0.1845,  0.0363, -0.0529,  0.0185,\n",
      "         -0.0268,  0.1259, -0.1698,  0.1268,  0.1409, -0.1515,  0.2041,  0.2455]],\n",
      "       requires_grad=True) \n",
      "\n",
      "Layer: neural_network.0.bias | Size: torch.Size([16]) | Values : Parameter containing:\n",
      "tensor([-0.0986, -0.0700,  0.1246, -0.1959,  0.0989,  0.0583,  0.1576, -0.2172,\n",
      "        -0.2077,  0.1123,  0.0901, -0.2327, -0.1394, -0.2287, -0.1791, -0.1128],\n",
      "       requires_grad=True) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build the neural network\n",
    "\n",
    "output_layer_nodes = train_data.y.shape[1]\n",
    "input_layer_nodes = train_data.X.shape[1]\n",
    "bias = True\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "model = net(input_layer_nodes, output_layer_nodes, bias=bias).to(device) # linear-relu-linear-softwax nn (1 hidden layer)\n",
    "print(f\"Model structure: {model}\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([6])\n",
      "torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "# example to show how model is used from prediction\n",
    "\n",
    "X = torch.rand(1, input_layer_nodes, device=device)\n",
    "logits = model(X)\n",
    "# pred_probab = nn.Softmax(dim=1)(logits)\n",
    "# y_pred = pred_probab.argmax(1)\n",
    "y_pred = logits.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: \"WANDB_NOTEBOOK_NAME\"=\"3b_Deep Learning.ipynb\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\gille\\OneDrive\\1-Projects\\_Horse Racing 2H22\\New Framework\\wandb\\run-20230422_133133-v2i4kqzl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gillenpj/horse-racing-project/runs/v2i4kqzl' target=\"_blank\">major-spaceship-63</a></strong> to <a href='https://wandb.ai/gillenpj/horse-racing-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gillenpj/horse-racing-project' target=\"_blank\">https://wandb.ai/gillenpj/horse-racing-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gillenpj/horse-racing-project/runs/v2i4kqzl' target=\"_blank\">https://wandb.ai/gillenpj/horse-racing-project/runs/v2i4kqzl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.756482  [   64/16620]\n",
      "loss: 2.630939  [ 6464/16620]\n",
      "loss: 2.609127  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 23.3%, Avg loss: 2.583585 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.646704  [   64/16620]\n",
      "loss: 2.559203  [ 6464/16620]\n",
      "loss: 2.569918  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 23.5%, Avg loss: 2.558491 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.637134  [   64/16620]\n",
      "loss: 2.541376  [ 6464/16620]\n",
      "loss: 2.557241  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 23.2%, Avg loss: 2.547749 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.632822  [   64/16620]\n",
      "loss: 2.532133  [ 6464/16620]\n",
      "loss: 2.550077  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 22.9%, Avg loss: 2.540634 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.629038  [   64/16620]\n",
      "loss: 2.525591  [ 6464/16620]\n",
      "loss: 2.544805  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 22.4%, Avg loss: 2.534874 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.625226  [   64/16620]\n",
      "loss: 2.520219  [ 6464/16620]\n",
      "loss: 2.540392  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 22.0%, Avg loss: 2.529740 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.621340  [   64/16620]\n",
      "loss: 2.515478  [ 6464/16620]\n",
      "loss: 2.536411  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 21.4%, Avg loss: 2.524928 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.617387  [   64/16620]\n",
      "loss: 2.511153  [ 6464/16620]\n",
      "loss: 2.532629  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 21.0%, Avg loss: 2.520279 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 2.613381  [   64/16620]\n",
      "loss: 2.507166  [ 6464/16620]\n",
      "loss: 2.528910  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 20.6%, Avg loss: 2.515693 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 2.609346  [   64/16620]\n",
      "loss: 2.503477  [ 6464/16620]\n",
      "loss: 2.525166  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 20.4%, Avg loss: 2.511102 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 2.605315  [   64/16620]\n",
      "loss: 2.500036  [ 6464/16620]\n",
      "loss: 2.521342  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 20.5%, Avg loss: 2.506459 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 2.601317  [   64/16620]\n",
      "loss: 2.496757  [ 6464/16620]\n",
      "loss: 2.517395  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 21.1%, Avg loss: 2.501750 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 2.597357  [   64/16620]\n",
      "loss: 2.493525  [ 6464/16620]\n",
      "loss: 2.513281  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 22.5%, Avg loss: 2.496991 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 2.593400  [   64/16620]\n",
      "loss: 2.490215  [ 6464/16620]\n",
      "loss: 2.508979  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 23.9%, Avg loss: 2.492223 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 2.589392  [   64/16620]\n",
      "loss: 2.486723  [ 6464/16620]\n",
      "loss: 2.504511  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 25.9%, Avg loss: 2.487486 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 2.585293  [   64/16620]\n",
      "loss: 2.482989  [ 6464/16620]\n",
      "loss: 2.499931  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 28.1%, Avg loss: 2.482810 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 2.581093  [   64/16620]\n",
      "loss: 2.479021  [ 6464/16620]\n",
      "loss: 2.495303  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 30.5%, Avg loss: 2.478222 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 2.576829  [   64/16620]\n",
      "loss: 2.474902  [ 6464/16620]\n",
      "loss: 2.490686  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 32.2%, Avg loss: 2.473751 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 2.572580  [   64/16620]\n",
      "loss: 2.470764  [ 6464/16620]\n",
      "loss: 2.486131  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 33.2%, Avg loss: 2.469421 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 2.568434  [   64/16620]\n",
      "loss: 2.466717  [ 6464/16620]\n",
      "loss: 2.481680  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 34.0%, Avg loss: 2.465247 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 2.564440  [   64/16620]\n",
      "loss: 2.462810  [ 6464/16620]\n",
      "loss: 2.477361  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 34.4%, Avg loss: 2.461239 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 2.560604  [   64/16620]\n",
      "loss: 2.459046  [ 6464/16620]\n",
      "loss: 2.473196  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 34.8%, Avg loss: 2.457404 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 2.556921  [   64/16620]\n",
      "loss: 2.455419  [ 6464/16620]\n",
      "loss: 2.469196  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 34.8%, Avg loss: 2.453743 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 2.553385  [   64/16620]\n",
      "loss: 2.451925  [ 6464/16620]\n",
      "loss: 2.465364  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 2.450251 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 2.549991  [   64/16620]\n",
      "loss: 2.448560  [ 6464/16620]\n",
      "loss: 2.461700  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 2.446924 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 2.546736  [   64/16620]\n",
      "loss: 2.445320  [ 6464/16620]\n",
      "loss: 2.458199  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 2.443754 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 2.543617  [   64/16620]\n",
      "loss: 2.442203  [ 6464/16620]\n",
      "loss: 2.454855  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 34.9%, Avg loss: 2.440734 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 2.540628  [   64/16620]\n",
      "loss: 2.439204  [ 6464/16620]\n",
      "loss: 2.451661  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.0%, Avg loss: 2.437856 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 2.537767  [   64/16620]\n",
      "loss: 2.436318  [ 6464/16620]\n",
      "loss: 2.448612  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.0%, Avg loss: 2.435110 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 2.535026  [   64/16620]\n",
      "loss: 2.433542  [ 6464/16620]\n",
      "loss: 2.445699  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.432491 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 2.532400  [   64/16620]\n",
      "loss: 2.430869  [ 6464/16620]\n",
      "loss: 2.442915  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.429990 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 2.529884  [   64/16620]\n",
      "loss: 2.428295  [ 6464/16620]\n",
      "loss: 2.440254  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.0%, Avg loss: 2.427600 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 2.527471  [   64/16620]\n",
      "loss: 2.425815  [ 6464/16620]\n",
      "loss: 2.437708  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.425315 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 2.525157  [   64/16620]\n",
      "loss: 2.423424  [ 6464/16620]\n",
      "loss: 2.435271  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 2.423127 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 2.522936  [   64/16620]\n",
      "loss: 2.421118  [ 6464/16620]\n",
      "loss: 2.432936  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 2.421032 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 2.520802  [   64/16620]\n",
      "loss: 2.418893  [ 6464/16620]\n",
      "loss: 2.430698  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.419024 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 2.518752  [   64/16620]\n",
      "loss: 2.416744  [ 6464/16620]\n",
      "loss: 2.428550  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.417097 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 2.516780  [   64/16620]\n",
      "loss: 2.414668  [ 6464/16620]\n",
      "loss: 2.426489  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.415246 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 2.514883  [   64/16620]\n",
      "loss: 2.412660  [ 6464/16620]\n",
      "loss: 2.424508  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.413468 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 2.513055  [   64/16620]\n",
      "loss: 2.410718  [ 6464/16620]\n",
      "loss: 2.422604  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.411758 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 2.511294  [   64/16620]\n",
      "loss: 2.408838  [ 6464/16620]\n",
      "loss: 2.420772  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 2.410112 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 2.509595  [   64/16620]\n",
      "loss: 2.407017  [ 6464/16620]\n",
      "loss: 2.419008  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.408526 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 2.507956  [   64/16620]\n",
      "loss: 2.405252  [ 6464/16620]\n",
      "loss: 2.417308  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.1%, Avg loss: 2.406998 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 2.506373  [   64/16620]\n",
      "loss: 2.403541  [ 6464/16620]\n",
      "loss: 2.415668  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 2.405524 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 2.504844  [   64/16620]\n",
      "loss: 2.401881  [ 6464/16620]\n",
      "loss: 2.414087  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.2%, Avg loss: 2.404102 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 2.503364  [   64/16620]\n",
      "loss: 2.400269  [ 6464/16620]\n",
      "loss: 2.412559  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.3%, Avg loss: 2.402728 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 2.501933  [   64/16620]\n",
      "loss: 2.398704  [ 6464/16620]\n",
      "loss: 2.411084  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.3%, Avg loss: 2.401400 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 2.500548  [   64/16620]\n",
      "loss: 2.397183  [ 6464/16620]\n",
      "loss: 2.409657  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.3%, Avg loss: 2.400116 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 2.499205  [   64/16620]\n",
      "loss: 2.395704  [ 6464/16620]\n",
      "loss: 2.408277  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.3%, Avg loss: 2.398873 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 2.497904  [   64/16620]\n",
      "loss: 2.394266  [ 6464/16620]\n",
      "loss: 2.406941  [12864/16620]\n",
      "Test Error: \n",
      " Accuracy: 35.4%, Avg loss: 2.397670 \n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0cc3d2924304b53992ed50cc7bbeccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>▂▂▂▂▂▁▁▁▁▁▂▃▅▆▇▇████████████████████████</td></tr><tr><td>loss</td><td>█▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>acc</td><td>0.35414</td></tr><tr><td>loss</td><td>2.39767</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">major-spaceship-63</strong> at: <a href='https://wandb.ai/gillenpj/horse-racing-project/runs/v2i4kqzl' target=\"_blank\">https://wandb.ai/gillenpj/horse-racing-project/runs/v2i4kqzl</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230422_133133-v2i4kqzl\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%env \"WANDB_NOTEBOOK_NAME\" \"3b_Deep Learning.ipynb\"\n",
    "\n",
    "# optimizing model parameters\n",
    "\n",
    "# hyperparameters\n",
    "learning_rate = 10e-1\n",
    "epochs = 50\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"horse-racing-project\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"architecture\": list(model.modules()),\n",
    "    \"dataset\": [train_data_fn, test_data_fn],\n",
    "    \"epochs\": epochs,\n",
    "    \"bias\": bias,\n",
    "    \"device\": device,\n",
    "    \"X_columns\": X_columns\n",
    "    }\n",
    ")\n",
    "\n",
    "# initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
    "    (acc, loss) = test_loop(test_dataloader, model, loss_fn, device)\n",
    "    wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "print(\"Done!\")\n",
    "\n",
    "# finish the wandb run, necessary in notebooks\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for para_name, para_vals in model.named_parameters():\n",
    "    np.savetxt(para_name + \".csv\", para_vals.data.numpy(), fmt='%6.3f', delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4ed282fe5a96d451181bcb846a73bf3735bfa0d466ee57d09e22e537eb2d1df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
